{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text processing...\n",
      "Training the NBC...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "# Load in the training set .csv\n",
    "training_set = pd.read_csv(\"trg.csv\")\n",
    "training_set.head()\n",
    "\n",
    "\n",
    "# Process the text, find a 'good model' with cross-validation\n",
    "print(\"Text processing...\")\n",
    "\n",
    "#Extensions\n",
    "\n",
    "for i in range(4000):\n",
    "    for word in training_set['abstract'][i].split():\n",
    "        if len(word) > 13 or len(word) <= 3:\n",
    "            word.replace(word,'')\n",
    "            \n",
    "delete = ['1', '2', '3','4','5', '6', '7', '8', '9', '0', '-']\n",
    "for i in range(4000):\n",
    "    for word in training_set['abstract'][i].split():\n",
    "        if word in delete:\n",
    "            word.replace(word, '')\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Train the NBC with this data (your own NBC code)\n",
    "def train(c, abstract):\n",
    "    print(\"Training the NBC...\")\n",
    "    \n",
    "    \n",
    "    countA = 0\n",
    "    countB = 0\n",
    "    countE = 0\n",
    "    countV = 0\n",
    "\n",
    "    for i in range(4000):\n",
    "        if c[i] == 'A':\n",
    "            countA += 1\n",
    "        elif c[i] == 'B':\n",
    "            countB += 1\n",
    "        elif c[i] == 'E':\n",
    "            countE += 1\n",
    "        elif c[i] == 'V':\n",
    "            countV += 1\n",
    "\n",
    "\n",
    "\n",
    "    wordsA = []\n",
    "    wordsB = []\n",
    "    wordsE = []\n",
    "    wordsV = []\n",
    "\n",
    "    for i in range(4000):\n",
    "        for word in abstract[i].split():\n",
    "            if c[i] == \"A\" and len(word) < 13:\n",
    "                wordsA.append(word)\n",
    "            elif c[i] == \"B\" and len(word) < 13:\n",
    "                wordsB.append(word)\n",
    "            elif c[i] == \"E\" and len(word) < 13:\n",
    "                wordsE.append(word)\n",
    "            elif c[i] == \"V\" and len(word) < 13:\n",
    "                wordsV.append(word)\n",
    "\n",
    "    all_words = []\n",
    "    all_words = wordsA + wordsB + wordsE + wordsV\n",
    "    unique = list(dict.fromkeys(all_words))\n",
    "    #v = 32241\n",
    "    v = len(unique)\n",
    "\n",
    "    all_words.sort(key=Counter(all_words).get, reverse=True)\n",
    "    unique = list(dict.fromkeys(all_words))\n",
    "\n",
    "    delete_words = ['the', 'and', 'that', 'for','from', 'are', 'was','were', 'this', 'which', 'have', 'these', 'protein', 'with']\n",
    "\n",
    "    #unique.sort(key = lambda s: len(s))\n",
    "    #unique.reverse()\n",
    "    unique = [x for x in unique if not any(c.isdigit() for c in x)]\n",
    "    unique = [x for x in unique if len(x)>2]\n",
    "    unique = [x for x in unique if len(x)<26]\n",
    "    unique = [x for x in unique if x not in delete_words]\n",
    "\n",
    "\n",
    "    unique = unique[:1000]\n",
    "\n",
    "\n",
    "    pA = [1] * 1000\n",
    "    pB = [1] * 1000\n",
    "    pE = [1] * 1000\n",
    "    pV = [1] * 1000\n",
    "\n",
    "    for i in range(1000):\n",
    "        pA[i] = (wordsA.count(unique[i]) + 1) / (countA + v)\n",
    "        pB[i] = (wordsB.count(unique[i]) + 1) / (countB + v)\n",
    "        pE[i] = (wordsE.count(unique[i]) + 1) / (countE + v)\n",
    "        pV[i] = (wordsV.count(unique[i]) + 1) / (countV + v)\n",
    "\n",
    "    priorA = countA / 4000\n",
    "    priorB = countB / 4000\n",
    "    priorE = countE / 4000\n",
    "    priorV = countV / 4000\n",
    "\n",
    "\n",
    "# Use this 'good model' to generate classifications. \n",
    "def classify(abstracts):\n",
    "    \n",
    "    # Text processing, cleaning, outlier removal, attribute selection etc. \n",
    "    # This function must be deterministic \n",
    "    # eg. if you select the 100 most frequent words, it must be the 100 most frequent words in the TRAINING set not\n",
    "    # in the 'abstracts' parsed\n",
    "    print(\"Processing the test abstracts...\")\n",
    "    \n",
    "    \n",
    "    # Run processed abstracts through the pre-trained naive bayes classifier\n",
    "    print(\"Classifying the test abstracts...\")\n",
    "    \n",
    "    results = ['E'] * 1000\n",
    "    \n",
    "    for i in range(1000):\n",
    "        resA = countA / 4000\n",
    "        resB = countB / 4000\n",
    "        resE = countE / 4000\n",
    "        resV = countV / 4000\n",
    "        for word in abstracts[i].split():          \n",
    "            if word in unique and len(word) > 2 and len(word) < 25:            \n",
    "                resA *= pA[unique.index(word)]\n",
    "                resB *= pB[unique.index(word)]\n",
    "                resE *= pE[unique.index(word)]\n",
    "                resV *= pV[unique.index(word)]\n",
    "                                \n",
    "                if resA >= resB and resA >= resE and resA >= resV:\n",
    "                    results[i] = 'A'\n",
    "                elif resB >= resA and resB >= resE and resB >= resV:\n",
    "                    results[i] = 'B'\n",
    "                elif resE >= resA and resE >= resB and resE >= resV:\n",
    "                    results[i] = 'E'\n",
    "                elif resV >= resA and resV >= resE and resV >= resB:\n",
    "                    results [i] = 'V'\n",
    "    \n",
    "    \n",
    "    return results\n",
    "    \n",
    "    \n",
    "# Load in the test set .csv\n",
    "test_set = pd.read_csv(\"tst.csv\")\n",
    "\n",
    "#train model\n",
    "train(training_set['class'], training_set['abstract'])\n",
    "\n",
    "# Apply the model to the test set\n",
    "test_set_class_predictions = classify(test_set[\"abstract\"])\n",
    "test_set[\"class\"] = test_set_class_predictions\n",
    "\n",
    "\n",
    "# Write the test set classifications to a .csv so it can be submitted to Kaggle\n",
    "test_set.drop([\"abstract\"], axis = 1).to_csv(\"tst_kaggle.csv\", index=False)\n",
    "test_set.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
